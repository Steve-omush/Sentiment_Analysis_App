{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-19T18:59:24.479249Z",
     "start_time": "2025-10-19T18:59:24.473305Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Import Necessary Libraries",
   "id": "5bdb74e0455917d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:39:36.246567Z",
     "start_time": "2025-10-24T09:38:18.778873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Libraries\n",
    "from keras.src.legacy.preprocessing.text import tokenizer_from_json\n",
    "import numpy as np\n",
    "\n",
    "import re, string, nltk\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pickle"
   ],
   "id": "c3bf784ec95237be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:10:06.140870Z",
     "start_time": "2025-10-19T19:10:06.132433Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Preprocessing of tweets that is our data",
   "id": "33ad9aa9973c80a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:39:36.381339Z",
     "start_time": "2025-10-24T09:39:36.359609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def process_tweet(post):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "    post = re.sub(r'\\$\\w*', '', post)\n",
    "    post = re.sub(r'^RT[\\s]+', '', post)\n",
    "    post = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', post)\n",
    "    post = re.sub(r'#', '', post)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(post)\n",
    "\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stopwords_english and word not in string.punctuation\n",
    "    ]\n",
    "    return cleaned"
   ],
   "id": "cb62ee7069e0a90a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:45:59.325580Z",
     "start_time": "2025-10-19T19:45:59.317725Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Feature Set (Frequency of words)",
   "id": "3ab6cacfbc6f4d14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:39:36.533416Z",
     "start_time": "2025-10-24T09:39:36.513990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build a frequency dictionary for each word in tweets to train the model using Logistic Regression\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"\n",
    "    Build frequency dictionary for each word in tweets\n",
    "\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an array of labels for the tweets. an m x 1 array with the sentiment label of each tweet (either 0 or 1).\n",
    "\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # The list just needs to be a list, not a numpy array.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all words\n",
    "    # and over all labels (0, 1) to record the number of occurrences of each word\n",
    "    # with its corresponding label.\n",
    "    freqs = {}\n",
    "\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ],
   "id": "ccf5335362e95261",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:39:43.047152Z",
     "start_time": "2025-10-24T09:39:36.559593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the above code\n",
    "tweets = [\"I am happy\", \"I love this car\", \"I do not like this car\", \"I do not like the sound of the alarm\",  \"I am sleepy\", \"I am tired\", \"I am tired\"]\n",
    "ys = [1, 1, 0, 0, 1, 0, 0]\n",
    "res = build_freqs(tweets, ys)\n",
    "print(res)"
   ],
   "id": "ca3f0b67afaa47e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('happy', 1): 1, ('love', 1): 1, ('car', 1): 1, ('like', 0): 2, ('car', 0): 1, ('sound', 0): 1, ('alarm', 0): 1, ('sleepy', 1): 1, ('tired', 0): 2}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T20:18:55.714145Z",
     "start_time": "2025-10-19T20:18:55.705255Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Getting the Data",
   "id": "41a4ab55d4d7283b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:40:05.455681Z",
     "start_time": "2025-10-24T09:40:02.482113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('twitter_samples')\n",
    "# Select Post of negative and positive tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('The first tweet looks like this:\\n', all_positive_tweets[0])"
   ],
   "id": "ac1018da7ce5f530",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "The first tweet looks like this:\n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T20:19:12.795505Z",
     "start_time": "2025-10-19T20:19:12.785862Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Splitting the Data",
   "id": "e3872aeec8dd273a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:40:05.514321Z",
     "start_time": "2025-10-24T09:40:05.496962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Splitting the data into train and test\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "test_neg = all_negative_tweets[4000:]\n"
   ],
   "id": "26fa9ea8fe7c4bbb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:40:05.582637Z",
     "start_time": "2025-10-24T09:40:05.574210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg"
   ],
   "id": "ef9894a6d05834ed",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:40:05.649762Z",
     "start_time": "2025-10-24T09:40:05.611778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine positive and negative labels\n",
    "# Building the y - target variable here\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ],
   "id": "9decbdcd57c0b298",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:40:14.537233Z",
     "start_time": "2025-10-24T09:40:05.719731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "print(\"type(freqs) = \", type(freqs))\n",
    "print(\"len(freqs) = \", len(freqs))\n",
    "\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ],
   "id": "65d6bed5414c1abf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) =  <class 'dict'>\n",
      "len(freqs) =  12385\n",
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 12385\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T09:40:14.771938Z",
     "start_time": "2025-10-24T09:40:14.751439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[24])\n",
    "print('Label: ', train_y[24])\n",
    "\n",
    "print(\"\\n This is a processed version of the tweet: \\n\", process_tweet(train_x[24]))"
   ],
   "id": "32147c443e66effb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " ðŸ’…ðŸ½ðŸ’‹ - :)))) haven't seen you in years\n",
      "Label:  [1.]\n",
      "\n",
      " This is a processed version of the tweet: \n",
      " ['ðŸ’…ðŸ½', 'ðŸ’‹', ':)', 'seen', 'year']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T19:56:24.881741Z",
     "start_time": "2025-10-22T19:56:24.872239Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Building the Logistic Regression Model",
   "id": "9e0eb330035192ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:16:46.788723Z",
     "start_time": "2025-10-24T13:16:46.762662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    \"\"\"\n",
    "    zz = np.negative(z)\n",
    "    h = 1 / (1 + np.exp(zz))\n",
    "    return h"
   ],
   "id": "46ea6e6824354948",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:17:52.624589Z",
     "start_time": "2025-10-24T13:17:52.590788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cost Function and Gradient\n",
    "def gradient_descent(x, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x: matrix of features which is (m, n+1)\n",
    "        y: vector of labels. corresponding labels of the input matrix x, dimensions (m, 1)\n",
    "        theta: vector of parameters. weight vector of dimension (n+1, 1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        theta: vector of parameters. final weight vector after training\n",
    "        J: vector of cost for each iteration.  the final cost is J[-1]\n",
    "    Hint: might want to print the cost to make sure that it is going down\n",
    "    \"\"\"\n",
    "    # m = x.shape[0]\n",
    "    #\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    for i in range(0, num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        # calculate the cost function\n",
    "        cost = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n",
    "\n",
    "    cost = float(cost)\n",
    "    return cost, theta\n",
    "\n"
   ],
   "id": "c557eaf1db00eb7d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T10:20:18.296032Z",
     "start_time": "2025-10-24T10:20:18.273431Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Extracting the Features",
   "id": "cd613cad149705fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:18:54.364331Z",
     "start_time": "2025-10-24T13:18:54.340139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracting features\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    \"\"\"\n",
    "    :param tweet: a list of words for one tweet\n",
    "    :param freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    :return: x : a feature vector of dimension (1, n) where n = 3 (number of features)\n",
    "    \"\"\"\n",
    "    # process_tweet tokenizes the tweet\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros((1, 3))\n",
    "\n",
    "    # bias term is set to 1\n",
    "    x[0, 0] = 1\n",
    "\n",
    "    # loop through each word in the tweet\n",
    "    for word in word_l:\n",
    "        # increment the word count for the positive label 1\n",
    "        x[0, 1] += freqs.get((word, 1.0), 0)\n",
    "        # increment the word count for the negative label 0\n",
    "        x[0, 2] += freqs.get((word, 0.0), 0)\n",
    "\n",
    "    assert (x.shape == (1, 3))\n",
    "    return x"
   ],
   "id": "18c2e8ce2f8ca92d",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:19:10.621350Z",
     "start_time": "2025-10-24T13:19:09.867225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test on training data\n",
    "tmp1 = extract_features(train_x[22], freqs)\n",
    "print(tmp1)"
   ],
   "id": "c913852d53a287dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.004e+03 1.200e+02]]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:18:57.427214Z",
     "start_time": "2025-10-24T12:18:57.410533Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Train the Model",
   "id": "bab138ea38039fec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:20:44.570208Z",
     "start_time": "2025-10-24T13:20:21.906818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the model\n",
    "\n",
    "# Collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "# these values are predefined (Andrew NG)\n",
    "J, theta = gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n"
   ],
   "id": "18359881c980a06b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_2800\\1808146942.py:40: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cost = float(cost)\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:35:44.050889Z",
     "start_time": "2025-10-24T12:35:44.034487Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Predict Tweets",
   "id": "3d367bc38e420227"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:24:35.186899Z",
     "start_time": "2025-10-24T13:24:35.173274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output:\n",
    "        y_pred: the probability of a tweet being positive or negative (float)\n",
    "    \"\"\"\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "\n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "\n",
    "    return y_pred\n"
   ],
   "id": "5e4e2d4bc8209f81",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:24:41.095165Z",
     "start_time": "2025-10-24T13:24:41.070057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Logistic Regression\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "\n",
    "    :param test_x: a list of tweets\n",
    "    :param test_y: (m,1) vector with the corresponding label of each tweet\n",
    "    :param freqs: a dictioanry with each frequency of each pair (or tuple)\n",
    "    :param theta: weight vector of dimension (3,1)\n",
    "    :return: accuracy: (number of tweets classified correctly) / (total number of tweets)\n",
    "    \"\"\"\n",
    "\n",
    "    # list for stroring predictions\n",
    "    y_hat = []\n",
    "\n",
    "    for tweet in test_x:\n",
    "        # get label prediction for one tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # calculate the accuracy\n",
    "    accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
    "    return accuracy"
   ],
   "id": "d6e320faa5244d9f",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:24:46.169621Z",
     "start_time": "2025-10-24T13:24:42.761455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the function\n",
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ],
   "id": "963fc013accce3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9970\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:23:36.015863Z",
     "start_time": "2025-10-24T13:23:35.984796Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Predict with my own tweet",
   "id": "bfc28c164c31c54f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:26:24.680728Z",
     "start_time": "2025-10-24T13:26:24.668366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict\n",
    "\n",
    "def pre(sentence):\n",
    "    yhat = predict_tweet(sentence, freqs, theta)\n",
    "\n",
    "    if yhat > 0.5:\n",
    "        return \"Positive sentiment\"\n",
    "    elif yhat == 0:\n",
    "        return \"Neutral sentiment\"\n",
    "    else:\n",
    "        return \"Negative sentiment\""
   ],
   "id": "bef82419cc06fd01",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:28:59.208983Z",
     "start_time": "2025-10-24T13:28:59.197798Z"
    }
   },
   "cell_type": "code",
   "source": "my_tweet = \"Today is my dad\\'s funeral\"",
   "id": "9a315df298e337bc",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:28:59.996096Z",
     "start_time": "2025-10-24T13:28:59.980885Z"
    }
   },
   "cell_type": "code",
   "source": "res = pre(my_tweet)\n",
   "id": "6f6c23b9024aa57a",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:29:00.651465Z",
     "start_time": "2025-10-24T13:29:00.637507Z"
    }
   },
   "cell_type": "code",
   "source": "res",
   "id": "19356ce64573c483",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative sentiment'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f566b98c731b461"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
